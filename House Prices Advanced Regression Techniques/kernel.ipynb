{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# stacking이란 여러 모델을 만들고 이를 계층화시켜 그 장단점을 활용하는 머신러닝기법인듯. 여기서 참조한 코드는 이 stacking을 이용해 regression한듯.\n\n#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory\n\n\n# 1. intro 작업\n# 여기까지 pandas로 train과 test 읽어서 각각 train, test로 만듬. 그리고 colunm을 분석해보니 81개, 그리고 test는 prrice제외한 80개 나옴.\n# 그 다음 id는 분석하는데 필요없으니 따로 save해두고 train과 test에서 id를 drop함.\n#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n##display the first five rows of the train dataset.\ntrain.head(5)\n\n#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))\n\n\nprint(train['YrSold'])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 2. Data Processing\n# (1) outliare들을 제거하는 작업이 필요. \n# GrLivArea는 Above grade (ground) living area square feet 지상의 거주가능면적을 뜻하는 듯?\n# 이 변수와 feature간의 관계를 분석해보니 아름다운 비례관계를 무너뜨리는 우촉하단의 outliar 2개 발견\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n\n# GrLivArea가 4000 이상이고 가격이 300000 밑에 이쓴ㄴ ourliar제거\n#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n\n# 이 outliers들은 매우 안 좋은 분포를 보이기에 지웠지만 모든 outlier들을 지우는 것은 모들의 예측성을 저하시킬 수 있다. test에도 outlier는 있을테니\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f49086f0bbaabfa8155dc865eb37eab01be8622a"
      },
      "cell_type": "code",
      "source": "# (2) target variables\n#  1) sale price (예상해야하는 변수니 가장 먼저 분석해야)\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function  ~ 표준편차(mu)와 평균(sigma)를 norm이라는 라이브러리로 얻은 듯.\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution ~ 얻은 표준편차와 평균으로 시각화\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\n# 이렇게 시각화한 자료들을 살펴보면 가격이 균일하지 못하고 정규분표와 같은 그래프가 되지 못함.\n# 모델들은 정규분포화된 데이터를 좋아하니 이 비뚤어짐(skewed)를 수정할 필요있음.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1cd30e5e6889eb9ca0e8f30a7f05eec42919637a"
      },
      "cell_type": "code",
      "source": "#  2) Log-transformation of the target variable(목표로 하는 price 변수를 로그변화)\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n# 이 log1p를 찾아보니 1+x(대입한 값)의 자연로그를 반환하는 거라고함. \n# 음 통계적인 부분이라 정확히는 모르겠지만 심하게 skewed된 멱함수(power law function) 분포를 띠는 데이터를 정규분포(normal distribution) 로\n# 변환할 때 로그 변환 (log transformation)을 사용하곤 하는데 이게 그 도구라고함.\n\n\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n# 수정해서 얻은 salesprice의 분포 그래프는 매우 균일한 걸 알 수 있음. \n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n# 이렇게 자연로그 형태로 수정한 분포의 표준편차, 평균을 빼고. \n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n# 비례를 이루는지 그래프로 다시 한 번 확인",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d4b3bfb81b538a8b015188f96aab7c133dfd5763"
      },
      "cell_type": "code",
      "source": "# (3) feature enginnering\nntrain = train.shape[0]  # column말고 다른 축인 data수를 제외한 값.\nntest = test.shape[0]\ny_train = train.SalePrice.values # train에서 가격을 빼고.\nall_data = pd.concat((train, test)).reset_index(drop=True)  # train과 test를 합쳐서 다음줄에 saleprice를 뺌.\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "154eb843bd17595487be1f87e618f15b44814f35"
      },
      "cell_type": "code",
      "source": "#  1) Missing Data 파악\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100 # null인 data 수를 길이로 나논거네\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30] # null이 하나도 없는 값은 빼고\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})  # 최종적으로 null인 비율을 feature별로 나타낸 것. \n\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\n# 그래프로도 보여줌. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de83f66a3dca6ec225329b42aa88b817ab888626"
      },
      "cell_type": "code",
      "source": "# 2) Data 상관관계 파악\n#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)  #feature가 너무 많아서 heatmap으로 나타내니 보기 힘듬. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "685a5b9f7d218f92029171c2d2428e857e1e7817"
      },
      "cell_type": "code",
      "source": "# 3) missing value 넣기\n# PoolQC은 주변에 거지가 없다는 것을 의미하는 변수인데 이 feature는 거의 99퍼 null임. 사실 집 근처에 거지가 있는 경우는 거의 없으므로 \n# fillna함수를 이용해 누락된 값에 모두 None을 넣음\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n# 밑에 쥐가 있는지, 골목이 있는지, 담장이 있는지, 난로가 없는지도 모두 동일한 맥락에서 None넣음.\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n# 인접한 길의 직선거리는 인접 주택과 비슷하므로 이웃이 지는 값의 평균으로\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# 차고 관련 feature들은 매우작으므로 문자열로 나타내어지는 것들은 None, 숫자인 것들은 0으로. \nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\n# 지하관련 feature들도 차고 관련된 것드로가 마찬가지\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\n# 벽돌관련 feature도 마찬가지로\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n# 일반적인 구역분류(?) 라는 feature인데 이 feature은 대부분 RL이라서 첫 번째 MSZoing인 RL을 fillna함.\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# utilities는 두 개 말고는 allpub으로 통일되어있어서 유의미한 feature가 되지 못함. 싹 다 지우기\nall_data = all_data.drop(['Utilities'], axis=1)\n\n# TYP가 대부분. 그래서 NULL 이걸로 채우기.\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n# 대부분이 SBrkr이고 NA가 1개뿐이라 채우기\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n# 이것도 하나 비어있음.\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n# 이것도 하나 비어있음.\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n# WD가 대부분. 그래서 NULL 이걸로 채우기.\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n# BuildingClass가 높이를 뜻하는지 등급을 뜻하는지 명확하지 않지만 null이면 보통 none으로 봐야한다고 설명되어있음. \nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()\n# 남은 게 없음을 알 수 있음. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "162cb6a8131ed8ea6e6b56e7f1ed13662ff89225"
      },
      "cell_type": "code",
      "source": "# (4) more feature enginnering\n#  1) Transforming some numerical variables that are really categorical(범주화할 수 있는 변수들을 범주화하기)\n#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)       # str 타입으로 변하게하는 함수인 것 같은데 str 타입으로 바꾼다고 범주화가 가능한가? 흠\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39dd621f6230d4f3e3d202f76e550f5fdc06beb6"
      },
      "cell_type": "code",
      "source": "# 2) Label Encoding some categorical variables that may contain information in their ordering set\n# 범주화해서 label을 붙이는 전처리과정인 것 같음.\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8e813aae57e875b42a976a0d9f3f75a81d76bc0"
      },
      "cell_type": "code",
      "source": "# 3) Adding one more important feature\n# 공간 or 면적(area) 관련 feature은 매우 중요하므로 층별 area를 모두 합한 feature 만듬.\n# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a520d8f6d0d3b6a2b897f06bd7aaef7dd92f6b64"
      },
      "cell_type": "code",
      "source": "# 4) Skewed features\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n# feature들의 비뚤어진 정도를 나타내는 것 같은데 함수 찾아봐도 세세한 의미는 이해 안됨. \n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1899c0d5bdb43d669fd3da1f3ce99291ead23256"
      },
      "cell_type": "code",
      "source": "# 5) Box Cox Transformation of (highly) skewed features\n# 매우 왜곡된 feature들을 box cox 변형\n# Box-Cox 변환은 찾아보니 정규분포가 아닌 자료를 정규분포로 변환하기 위해 사용되는 변환기법이라고 함. \n# 4)에서 살펴본 skrewed정도가 심한 feature들을 변형하기 위한 방법인 듯. \nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35706899b9cb1cb51a450bc437725944a01a024d"
      },
      "cell_type": "code",
      "source": "# 6) Getting dummy categorical features\n# get_dummies를 통해 데이터프레임을 범주화할 수 있는 더미변수를 만듬.\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5eddeca95bdf206f45923191a187e321ebff7f0"
      },
      "cell_type": "code",
      "source": "# 새로운 train과 test 만들기. all_data가 pd.concat로 train과 test를 합쳐놓은 형태니 다시 분리하기.\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4502e2e204e0521183551de698d7d3c57196678"
      },
      "cell_type": "code",
      "source": "# 3. Modeling(모델링!)\n# 필요한 라이브러리 import하고\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# (1) Validation function. cross validation하기 위해서 data값들을 섞고 5개 fold로 validation하게 됨. \nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n# (2) 기본적인 모델들(여기서는 staked regression할테니 )\n#  1) LASSO Regression. 근데 lasso는 outlier에 취약해서 RobustScaler로 전처리해줌.\n# lasso자체는 오버피팅을 막기 위해 가중치의 절대값을 최소화하는 시그마를 추가적으로 더해줌. \n# https://datascienceschool.net/view-notebook/83d5e4fff7d64cb2aecfd7e42e1ece5e/\n# http://rpago.tistory.com/59 ~ 이 사이트들에 릿지, 랏소, 엘라스틱 넷 회귀에 대한 내용있는데 이건 회귀계산에 대한 통계적 지식없으면...\n# 찾아보니 중앙값(median)과 IQR(interquartile range)을 사용해 아웃라이어의 영향을 최소화해주는 함수라고 함. \nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n# 2) Elastic Net Regression. 이것도 똑같이 전처리\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n# 3) Kernel Ridge Regression\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# 4) Gradient Boosting Regression : outlier에 강한 huber loss 사용. \n# gradient boosting 회귀 트리는 random forest같이 decision tree를 변형한 거라고 함. decision tree는 과잉적합되기 쉬운 취약점이 있으므로 그 약점을\n# 보완하는 모델 중 하나인데 이 방법은 random forest와 같이 여러 개의 결정 트리를 묶어 모델을 만들지만 랜덤이 아니라 이전 트리의 오차를 보완하는 방식으로\n# 순차적으로 모델을 만든다고 함. 따라서 무작위성이 없고 사전 가지치기가 사용되며 하나에서 다섯개정도의 깊지 않은 트리 사용. \n# huber loss는 outlier에 강한 손실함수라고 하는데 robust regression에 쓰인다고 함.\n# 통계용어라 정확히는 인지 못해도 아웃라이어에 대비하기 위해 사용한 함수도구 정도로 이해하면 될 듯.\n#ㅡ기계학습에서 부스팅(Boosting)이란 단순하고 약한 학습기(Weak Learner)를 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만드는 방식을 의미한다.\n# 정확도가 낮더라도 일단 모델을 만들고, 드러난 약점(예측 오류)은 두 번째 모델이 보완한다. \n# 이 둘을 합치면 처음보다는 정확한 모델이 만들어지고, 그럼에도 여전히 남아 있는 문제는 다음 모델에서 보완하여 계속 더하는 과정을 반복하는 원리\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\n# 5) XGBoost~ gradient boosting을 이용한 라이브러리라고 함. \nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n# 6) LightGBM ~ xgboost와 비슷한 개념의 라이브러리인듯.\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n# 각 모델별로 score 내보기\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cc2b65fcfcc009b279e123253fcc3452a9ee96a"
      },
      "cell_type": "code",
      "source": "# (3) Stacking models방법! \n# 1) Simplest Stacking approach : Averaging base models\n# stacked model기법을 위해 각 모델별 평균구할 것. 모델별 코드의 재사용화, 캡슐화, 상속을 위해 클래스 만듬.\n\n# Averaged base models class\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n\n# Averaged base models score ~ 밑에 집어넣은 네 방법의 단순평균을 구할 것!\naveraged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9499198b19ee02ed2a66b181aa1c912cc3694178"
      },
      "cell_type": "code",
      "source": "# 2) Less simple Stacking : Adding a Meta-model ~ 후 솔직히 이해 안됨. \n# cross validation처럼 부분별로 나눠서 각 부분을 모델링 한다음 예측하는 것을 학습시키고 마지막에 평규내는 것 같은데..흠\n# 핵심은 meta model을 하나 정하고 다른 모델들이 prediction하는걸 meta model에다 학습시키는 것인듯.\n# Stacking averaged Models Class\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n# We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n       # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n\n    \n# Stacking Averaged models Score(이제 staking한 모델로 점수내보기)\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "134c770c2a810aeefd5c49863e6678a04622ceca"
      },
      "cell_type": "code",
      "source": "# 4. Ensembling StackedRegressor, XGBoost and LightGBM해서 마지막으로 prediction하기. \n# 먼저 evaluation함수 만들기. \ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred)) # mean_squared_error은 (실제값-예측값)**2라고함. 코드상에서 쓰는 측정방법\n# (1) stacked regressor로 구한 score\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))\n\nlasso.fit(train, y_train)\nlasso_result_train_pred = lasso.predict(train)\nlasso_result_pred = np.expm1(lasso.predict(test))\nprint(rmsle(y_train, lasso_result_train_pred))\n# (2) XGboost로\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))\n# (3) LightGBM로\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))\n# (4) 최종적으로 저 위의 3개를 ensemble한 값\n'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2d1e7663ce1d73af84dbc08a8bbace646fc12e6"
      },
      "cell_type": "code",
      "source": "print(rmsle(y_train,stacked_train_pred*0.40 +\n               xgb_train_pred*0.30+lgb_train_pred*0.30))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af6dbbd37b2d4a91a4d220c1c7905cc549fd6416"
      },
      "cell_type": "code",
      "source": "# ensemble해서\nensemble = stacked_pred*0.40 + xgb_pred*0.3 + lgb_pred*0.3\n# submission파일 만듬. \nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}